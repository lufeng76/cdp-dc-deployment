= 集群验证

**目录**

. <<HDFS>> +
.. <<HDFS without KRB+TLS> +
.. <<HDFS with KRB>>

== HDFS

=== HDFS without KRB+TLS

/data目录下存放了多个CSV文件（2018年法国交通事故统计表）。

....
# 以root用户登录节点feng-1

sudo -u hdfs hdfs dfs -mkdir /data
sudo -u hdfs hdfs dfs -chmod 777 /data
sudo -u hdfs hdfs dfs -chown etl_user:hadoop /data

sudo -u etl_user hdfs dfs -mkdir /data/caracteristiques
sudo -u etl_user hdfs dfs -mkdir /data/lieux
sudo -u etl_user hdfs dfs -mkdir /data/usagers
sudo -u etl_user hdfs dfs -mkdir /data/vehicules
sudo -u etl_user hdfs dfs -put lieux-2018.csv /data/lieux/
sudo -u etl_user hdfs dfs -put usagers-2018.csv /data/usagers/
sudo -u etl_user hdfs dfs -put vehicules-2018.csv /data/vehicules/
sudo -u etl_user hdfs dfs -put caracteristiques-2018.csv /data/caracteristiques
....

=== HDFS with KRB

....
kinit -kt etl_user.keytab etl_user/ccycloud-1.feng.root.hwx.site@FENG.COM
hdfs dfs -mkdir /data
hdfs hdfs dfs -chmod 777 /data
hdfs hdfs dfs -chown etl_user:hadoop /data
hdfs dfs -mkdir /data/caracteristiques
hdfs dfs -mkdir /data/lieux
hdfs dfs -mkdir /data/usagers
hdfs dfs -mkdir /data/vehicules
hdfs dfs -put lieux-2018.csv /data/lieux/
hdfs dfs -put usagers-2018.csv /data/usagers/
hdfs dfs -put vehicules-2018.csv /data/vehicules/
hdfs dfs -put caracteristiques-2018.csv /data/caracteristiques
....

== Hive

=== 非安全集群

[source,bash]
----

# 以root用户登录节点feng-1

beeline -n etl_user -p xxxx <<EOF

CREATE DATABASE accidents;
USE accidents;

CREATE EXTERNAL TABLE IF NOT EXISTS carac_csv
(Num_Acc BIGINT,an STRING,mois STRING,jour STRING,hrmn STRING,lum INT,agg INT,inter INT,atm INT,col INT,com STRING,adr STRING,gps STRING,lat INT,longi INT,dep INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:///data/caracteristiques/'
TBLPROPERTIES ("skip.header.line.count"="1");

CREATE TABLE IF NOT EXISTS carac (Num_Acc BIGINT,an STRING,mois STRING,jour STRING,hrmn STRING,lum INT,agg INT,inter INT,atm INT,col INT,com STRING,adr STRING,gps STRING,lat INT,longi INT,dep INT);

INSERT INTO carac SELECT * FROM carac_csv;



CREATE EXTERNAL TABLE IF NOT EXISTS lieux_csv
(Num_Acc BIGINT, catr INT,voie INT,v1 INT,v2 STRING,circ INT,nbv INT,pr INT,pr1 INT,vosp INT,prof INT,plan INT,lartpc INT,larrout INT,surf INT,infra INT,situ INT,env1 INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:///data/lieux/'
TBLPROPERTIES ("skip.header.line.count"="1");

CREATE TABLE IF NOT EXISTS lieux (Num_Acc BIGINT, catr INT,voie INT,v1 INT,v2 STRING,circ INT,nbv INT,pr INT,pr1 INT,vosp INT,prof INT,plan INT,lartpc INT,larrout INT,surf INT,infra INT,situ INT,env1 INT);

INSERT INTO lieux SELECT * FROM lieux_csv;



CREATE EXTERNAL TABLE IF NOT EXISTS usagers_csv
(Num_Acc BIGINT, place INT,catu INT,grav INT,sexe INT,trajet INT,secu INT,locp INT,actp INT,etatp INT,an_nais INT,num_veh STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:///data/usagers/'
TBLPROPERTIES ("skip.header.line.count"="1");

CREATE TABLE IF NOT EXISTS usagers (Num_Acc BIGINT, place INT,catu INT,grav INT,sexe INT,trajet INT,secu INT,locp INT,actp INT,etatp INT,an_nais INT,num_veh STRING);

INSERT INTO usagers SELECT * FROM usagers_csv;



CREATE EXTERNAL TABLE IF NOT EXISTS vehicules_csv
(Num_Acc BIGINT, senc INT,catv INT,occutc INT,obs INT,obsm INT,choc INT,manv INT,num_veh STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION 'hdfs:///data/vehicules/'
TBLPROPERTIES ("skip.header.line.count"="1");

CREATE TABLE IF NOT EXISTS vehicules (Num_Acc BIGINT, senc INT,catv INT,occutc INT,obs INT,obsm INT,choc INT,manv INT,num_veh STRING);

INSERT INTO vehicules SELECT * FROM vehicules_csv;

EOF
----

=== 安全集群

将执行命令换成：

[source,bash]
kinit -kt etl_user.keytab etl_user/ccycloud-1.feng.root.hwx.site@FENG.COM
beeline -u "jdbc:hive2://ccycloud-1.feng.root.hwx.site:10099/default;principal=hive/ccycloud-1.feng.root.hwx.site@FENG.COM"

== Impala/Kudu

=== 非安全集群

建立一张kudu表，然后插入数据

[source,bash]
----

# 以root用户登录节点feng-1

impala-shell -i ccycloud-1.feng.root.hwx.site:21001 << EOF

CREATE DATABASE impala_kudu;

CREATE TABLE IF NOT EXISTS impala_kudu.usagers 
(Num_Acc BIGINT, place INT,catu INT,grav INT,sexe INT,trajet INT,secu INT,locp INT,actp INT,etatp INT,an_nais INT,
num_veh STRING,
PRIMARY KEY(Num_Acc))
PARTITION BY HASH PARTITIONS 16
STORED AS kudu;

INSERT INTO impala_kudu.usagers SELECT * FROM accidents.usagers_csv;

EOF
----

=== 安全集群

将执行命令换成：

[source,bash]
kinit -kt etl_user.keytab etl_user/ccycloud-1.feng.root.hwx.site@FENG.COM
impala-shell -V -k -i ccycloud-1.feng.root.hwx.site:21001

== Kafka

=== 非安全集群

[source,bash]
----

# 以root用户登录节点feng-1

export KAFKA_ZNODE=/kafka
export KAFKA_TOPIC=mytopic
export KAFKA_BROKERS=ccycloud-3.feng.root.hwx.site:9092,ccycloud-4.feng.root.hwx.site:9092,ccycloud-5.feng.root.hwx.site:9092
export KAFKA_BOOTSTRAP_BROKER=$(echo ${KAFKA_BROKERS} | awk -F',' '{print $1}')
export KAFKA_CONSUMER_GROUP=myconsumergroup
export ZOOKEEPER_QUORUM=ccycloud-1.feng.root.hwx.site:2181,ccycloud-2.feng.root.hwx.site:2181,ccycloud-3.feng.root.hwx.site:2181

kafka-topics --list --zookeeper ${ZOOKEEPER_QUORUM}${KAFKA_ZNODE}

kafka-topics --create --zookeeper ${ZOOKEEPER_QUORUM}${KAFKA_ZNODE} --topic ${KAFKA_TOPIC} --replication-factor 1 --partitions 1

kafka-topics --describe --zookeeper ${ZOOKEEPER_QUORUM}${KAFKA_ZNODE} --topic ${KAFKA_TOPIC}

while true; do echo "$(( RANDOM % 10 )),$(echo ${RANDOM} | tr '[0-9]' '[a-zA-Z]')"; sleep 5; done | kafka-console-producer --broker-list ${KAFKA_BROKERS} --topic ${KAFKA_TOPIC}

kafka-console-consumer --bootstrap-server ${KAFKA_BOOTSTRAP_BROKER} --topic ${KAFKA_TOPIC} --from-beginning --group ${KAFKA_CONSUMER_GROUP}

kafka-consumer-groups  --describe --group ${KAFKA_CONSUMER_GROUP} --bootstrap-server ${KAFKA_BOOTSTRAP_BROKER} 

----

=== 安全集群
[source,bash]
----

# 需要增加以下步骤

kinit -kt /etc/security/keytabs/etl_user.keytab etl_user/ccycloud-1.feng.root.hwx.site@FENG.COM

cat - > /tmp/jaas.conf << EOF 
KafkaClient {
com.sun.security.auth.module.Krb5LoginModule required
useTicketCache=true;
};
EOF

cat - > /tmp/client.properties << EOF
security.protocol=SASL_PLAINTEXT
sasl.kerberos.service.name=kafka
EOF

export KAFKA_OPTS="-Djava.security.auth.login.config=/tmp/jaas.conf"

while true; do echo "$(( RANDOM % 10 )),$(echo ${RANDOM} | tr '[0-9]' '[a-zA-Z]')"; sleep 5; done | kafka-console-producer --producer.config /tmp/client.properties --broker-list ${KAFKA_BROKERS} --topic ${KAFKA_TOPIC}

kafka-console-consumer --consumer.config /tmp/client.properties --bootstrap-server ${KAFKA_BOOTSTRAP_BROKER} --topic ${KAFKA_TOPIC} --from-beginning --group ${KAFKA_CONSUMER_GROUP}

----

== Hbase

=== 非安全集群

[source,bash]
----

# 以root用户登录节点feng-1

sudo -u hbase hbase ltt -tn test -write 1:10:10 -num_keys 100
sudo -u hbase hbase shell << EOF
list
describe 'test'
count 'test'
list_regions 'test'
EOF

----

=== 安全集群

[source,bash]
----
kinit -kt /etc/security/keytabs/hbase.keytab hbase/ccycloud-1.feng.root.hwx.site@FENG.COM
hbase ltt -tn test -write 1:10:10 -num_keys 100
hbase hbase shell << EOF
list
describe 'test'
count 'test'
list_regions 'test'
EOF
----

== SolR

In Java Program:

[source,bash]
----
HttpSolrClient httpSolrClient = new HttpSolrClient.Builder("http://"+PropertiesLoader.properties.getProperty("solr.server.url")+":"+
                PropertiesLoader.properties.getProperty("solr.server.port")+"/solr")
                .withConnectionTimeout(10000)
                .withSocketTimeout(60000)
                .build();


// Create SolR collection
 try {
     httpSolrClient.request(
             CollectionAdminRequest.createCollection(PropertiesLoader.properties.getProperty("solr.collection"),
                     Integer.valueOf(PropertiesLoader.properties.getProperty("solr.collection.shards")),
                     Integer.valueOf(PropertiesLoader.properties.getProperty("solr.collection.replicas")))
     );
 } catch(HttpSolrClient.RemoteSolrException e) {
     if(e.getMessage().contains("collection already exists")) {
         logger.warn("Collection already exists so it has not been created");
     } else {
         logger.error("Could not create SolR collection : " + PropertiesLoader.properties.getProperty("solr.collection")
                 + " due to error: ", e);
     }
 } catch (Exception e) {
     logger.error("Could not create SolR collection : " + PropertiesLoader.properties.getProperty("solr.collection")
             + " due to error: ", e);
 }
 // Set base URL directly to the collection, note that this is required
httpSolrClient.setBaseURL("http://"+PropertiesLoader.properties.getProperty("solr.server.url")+":"+
         PropertiesLoader.properties.getProperty("solr.server.port")+"/solr/"+PropertiesLoader.properties.getProperty("solr.collection"));

SolrInputDocument doc = new SolrInputDocument();
doc.addField("Value", row.toCSVString());

try {
    httpSolrClient.add(doc);
    httpSolrClient.commit();
} catch (Exception e) {
    logger.error("An unexpected error occurred while adding document: " + row.toString() + " to SolR collection : " +
            PropertiesLoader.properties.getProperty("solr.collection") + " due to error:", e);
}

httpSolrClient.close();
----

== Ozone

Using CLI:

[source,bash]
ozone sh volume create /test
ozone sh bucket create /test/bucket1
ozone sh key put /test/bucket1/hoster /etc/hosts
ozone sh key list /test/bucket1


Using Java Program:

[source,bash]
----
ozClient = OzoneClientFactory.getRpcClient(PropertiesLoader.properties.getProperty("ozone.om.uri"),
            Integer.valueOf(PropertiesLoader.properties.getProperty("ozone.om.port")));
objectStore = ozClient.getObjectStore();

// Create volume if not exists
     try {
    objectStore.createVolume(PropertiesLoader.properties.getProperty("ozone.volume.name"));
} catch (OMException e) {
    if(e.getResult() == OMException.ResultCodes.VOLUME_ALREADY_EXISTS) {
        logger.info("Volume: " + PropertiesLoader.properties.getProperty("ozone.volume.name") + " already exists ");
    } else {
        logger.error("An error occurred while creating volume " +
                PropertiesLoader.properties.getProperty("ozone.volume.name") + " : ", e);
    }
} catch (IOException e) {
    logger.error("An unexpected exception occurred while creating volume " +
            PropertiesLoader.properties.getProperty("ozone.volume.name") + ": ", e);
}

volume = objectStore.getVolume(PropertiesLoader.properties.getProperty("ozone.volume.name"));

// Create bucket if not exists
String bucketName = PropertiesLoader.properties.getProperty("ozone.bucket.prefix") + bucketNumber; 
volume.createBucket(bucketName);
OzoneBucket bucket = volume.getBucket(bucketName);

Random random = new Random();
byte[] blob = new byte[1_000_000];
random.nextBytes(blob);
OzoneOutputStream os = bucket.createKey(name+birthdate+country, blob.length);
os.write(blob);
os.close();

----

= 验证UI界面

== Hue

username: admin
password: admin

== Ranger

username: admin
password: Admin1234

== Atlas

username: admin
password: Admin1234


== PSQL Database

username: postgres  
password: Admin1234

== 其他UIs

不需要密码，但部分需要SPNEGO验证

